{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model To Detect objects"
      ],
      "metadata": {
        "id": "94rUjU4lUvsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-RFlBoQg_W8",
        "outputId": "41a6bb8c-3e80-4b13-8199-b0187d75c103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.39-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Downloading roboflow-1.1.39-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, requests-toolbelt, roboflow\n",
            "Successfully installed filetype-1.2.0 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.39\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.2.77, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Egyptian-ID-Detectr-3 to yolov8:: 100%|██████████| 19031/19031 [00:00<00:00, 27334.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Egyptian-ID-Detectr-3 in yolov8:: 100%|██████████| 748/748 [00:00<00:00, 3400.32it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"JiN3MQkW1CkxFGPpnE0K\")\n",
        "project = rf.workspace(\"omartamer0\").project(\"egyptian-id-detectr\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp1oHTKXhD0P",
        "outputId": "2f1622c4-3773-48ee-bb4d-b17a28ef79b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.25M/6.25M [00:00<00:00, 95.7MB/s]\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/Egyptian-ID-Detectr-3/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 24.6MB/s]\n",
            "Overriding model.yaml nc=80 with nc=31\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    757357  ultralytics.nn.modules.head.Detect           [31, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3,016,893 parameters, 3,016,877 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/train/labels... 333 images, 4 backgrounds, 0 corrupt: 100% 333/333 [00:00<00:00, 1969.58it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Egyptian-ID-Detectr-3/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/valid/labels... 23 images, 0 backgrounds, 0 corrupt: 100% 23/23 [00:00<00:00, 1351.39it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Egyptian-ID-Detectr-3/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000286, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      2.57G      1.774      4.614      1.589         89        640: 100% 21/21 [00:11<00:00,  1.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.06it/s]\n",
            "                   all         23        157          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      2.37G      1.178      4.157      1.171         94        640: 100% 21/21 [00:06<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.81s/it]\n",
            "                   all         23        157     0.0602      0.072     0.0627     0.0506\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      2.37G     0.9977      3.534      1.046         91        640: 100% 21/21 [00:04<00:00,  4.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.40it/s]\n",
            "                   all         23        157     0.0886      0.619      0.321      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      2.37G     0.9456      2.906      1.024        101        640: 100% 21/21 [00:06<00:00,  3.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.63it/s]\n",
            "                   all         23        157      0.719      0.394      0.386      0.287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      2.37G     0.8931      2.448      1.003         89        640: 100% 21/21 [00:04<00:00,  4.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.82it/s]\n",
            "                   all         23        157      0.673      0.572      0.544      0.397\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      2.37G     0.8731      2.129     0.9917         90        640: 100% 21/21 [00:04<00:00,  4.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.95it/s]\n",
            "                   all         23        157      0.501      0.661      0.587      0.429\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      2.37G     0.8586      1.881     0.9882         92        640: 100% 21/21 [00:07<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.79it/s]\n",
            "                   all         23        157      0.676      0.629      0.616      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      2.37G     0.8389      1.746     0.9784         76        640: 100% 21/21 [00:04<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.25it/s]\n",
            "                   all         23        157      0.539      0.828      0.707      0.531\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      2.37G     0.8287      1.666     0.9857         81        640: 100% 21/21 [00:07<00:00,  2.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.76it/s]\n",
            "                   all         23        157      0.729      0.682      0.757      0.577\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      2.37G     0.8124      1.583     0.9786         91        640: 100% 21/21 [00:04<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.91it/s]\n",
            "                   all         23        157      0.741      0.711      0.785      0.599\n",
            "\n",
            "10 epochs completed in 0.023 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,011,693 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.48it/s]\n",
            "                   all         23        157      0.734      0.717      0.785      0.598\n",
            "               address         10         10          1      0.771      0.995      0.861\n",
            "                  demo          9          9      0.874      0.773      0.951      0.725\n",
            "                   dob          9          9       0.74          1      0.906       0.72\n",
            "                expiry          9          9      0.931      0.667      0.912      0.722\n",
            "             firstName         10         10          1      0.775      0.972      0.613\n",
            "            front_logo          9          9      0.922          1      0.995      0.832\n",
            "       invalid_address          2          2      0.378          1      0.995      0.821\n",
            "          invalid_demo          2          2          1          0      0.133      0.119\n",
            "           invalid_dob          3          3      0.263          1      0.431      0.343\n",
            "        invalid_expiry          2          2          1          0      0.497      0.415\n",
            "     invalid_firstName          2          2      0.508          1      0.995      0.497\n",
            "         invalid_issue          2          2          1          0      0.448      0.368\n",
            "           invalid_job          2          2          1          0       0.12      0.116\n",
            "      invalid_lastName          2          2      0.322          1      0.995      0.647\n",
            "           invalid_nid          2          2      0.221          1      0.995       0.55\n",
            "      invalid_nid_back          2          2          0          0      0.199      0.145\n",
            "         invalid_photo          2          2      0.147        0.5      0.248      0.216\n",
            "        invalid_serial          3          3          1      0.553      0.995      0.636\n",
            " invalid_watermark_tut          2          2          1       0.98      0.995      0.846\n",
            "                 issue          9          9      0.863      0.778      0.946      0.785\n",
            "                   job          9          9       0.88      0.818      0.846      0.666\n",
            "              lastName         10         10      0.677        0.9      0.898      0.672\n",
            "                   nid         10         10      0.809      0.848      0.862       0.63\n",
            "              nid_back          9          9      0.859          1      0.984      0.832\n",
            "                 photo         10         10      0.807          1       0.94      0.827\n",
            "                serial          7          7      0.839          1      0.943      0.651\n",
            "         watermark_tut          9          9      0.785          1      0.995      0.893\n",
            "Speed: 0.4ms preprocess, 2.1ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!yolo task=detect mode=train model=yolov8n.pt data='/content/Egyptian-ID-Detectr-3/data.yaml' epochs=10 imgsz=640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KM9xwgbhgJ2",
        "outputId": "1d6c14de-cf65-4227-c44b-809e19ce3443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3,011,693 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/valid/labels.cache... 23 images, 0 backgrounds, 0 corrupt: 100%|██████████| 23/23 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         23        157      0.586        0.8      0.791      0.596\n",
            "               address         10         10          1      0.925      0.995      0.876\n",
            "                  demo          9          9      0.686      0.889      0.939      0.642\n",
            "                   dob          9          9      0.721          1      0.924      0.688\n",
            "                expiry          9          9      0.692          1      0.928      0.754\n",
            "             firstName         10         10          1      0.844      0.995      0.659\n",
            "            front_logo          9          9      0.833          1      0.984      0.839\n",
            "       invalid_address          2          2      0.317          1      0.995      0.796\n",
            "          invalid_demo          2          2          0          0      0.265      0.174\n",
            "           invalid_dob          3          3      0.246          1      0.555      0.449\n",
            "        invalid_expiry          2          2          1          0      0.497      0.386\n",
            "     invalid_firstName          2          2      0.513          1      0.995      0.547\n",
            "         invalid_issue          2          2          0          0      0.181      0.153\n",
            "           invalid_job          2          2      0.291        0.5      0.177      0.176\n",
            "      invalid_lastName          2          2      0.505          1      0.995      0.572\n",
            "           invalid_nid          2          2      0.176          1      0.995      0.502\n",
            "      invalid_nid_back          2          2          0          0      0.199       0.15\n",
            "         invalid_photo          2          2      0.115      0.591      0.338      0.305\n",
            "        invalid_serial          3          3      0.643          1      0.995      0.697\n",
            " invalid_watermark_tut          2          2      0.471          1      0.828      0.745\n",
            "                 issue          9          9      0.807          1      0.955      0.712\n",
            "                   job          9          9      0.884      0.848      0.836      0.622\n",
            "              lastName         10         10      0.942          1      0.995      0.709\n",
            "                   nid         10         10      0.831          1      0.939      0.725\n",
            "              nid_back          9          9      0.798          1      0.995      0.828\n",
            "                 photo         10         10        0.8          1      0.959      0.832\n",
            "                serial          7          7      0.779          1      0.924       0.64\n",
            "         watermark_tut          9          9      0.775          1      0.984      0.912\n",
            "Speed: 6.4ms preprocess, 388.9ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "Precision: 0.5862\n",
            "Recall: 0.7999\n",
            "mAP@50: 0.7914\n",
            "mAP@50-95: 0.5958\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model from the specific run\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "# Evaluate on the validation dataset\n",
        "results = model.val()\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Precision: {results.box.mp:.4f}\")  # Mean Precision\n",
        "print(f\"Recall: {results.box.mr:.4f}\")     # Mean Recall\n",
        "print(f\"mAP@50: {results.box.map50:.4f}\")  # Mean Average Precision at IoU 0.5\n",
        "print(f\"mAP@50-95: {results.box.map:.4f}\") # Mean Average Precision at IoU 0.5:0.95\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyperparameter tuning"
      ],
      "metadata": {
        "id": "97gq-KwMY-JY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7mvJdDKiNk6",
        "outputId": "7b603984-be96-41cd-ae84-bbef44d8a13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ 'mode' argument is missing. Valid modes are {'export', 'train', 'track', 'predict', 'benchmark', 'val'}. Using default 'mode=train'.\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/Egyptian-ID-Detectr-3/data.yaml, epochs=30, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Overriding model.yaml nc=80 with nc=31\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    757357  ultralytics.nn.modules.head.Detect           [31, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3,016,893 parameters, 3,016,877 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/train/labels.cache... 333 images, 4 backgrounds, 0 corrupt: 100% 333/333 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/valid/labels.cache... 23 images, 0 backgrounds, 0 corrupt: 100% 23/23 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000286, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/30      5.42G      1.869      4.738      1.661        174        640: 100% 11/11 [00:12<00:00,  1.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.69it/s]\n",
            "                   all         23        157          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/30      4.62G      1.437      4.534      1.377        155        640: 100% 11/11 [00:04<00:00,  2.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  6.00it/s]\n",
            "                   all         23        157          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/30      4.52G      1.134        4.2       1.14        174        640: 100% 11/11 [00:06<00:00,  1.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.24it/s]\n",
            "                   all         23        157     0.0905     0.0695     0.0837     0.0683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/30      4.64G      1.039      3.607      1.062        148        640: 100% 11/11 [00:04<00:00,  2.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.11it/s]\n",
            "                   all         23        157      0.103      0.421       0.23       0.18\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/30      4.54G      1.003      2.989      1.045        161        640: 100% 11/11 [00:05<00:00,  1.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.86it/s]\n",
            "                   all         23        157     0.0851      0.597      0.325      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/30      4.65G     0.9964      2.623      1.044        187        640: 100% 11/11 [00:04<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.57it/s]\n",
            "                   all         23        157      0.109       0.66      0.381      0.286\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/30      4.54G     0.9587      2.292      1.038        146        640: 100% 11/11 [00:04<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.09it/s]\n",
            "                   all         23        157      0.756      0.312      0.497       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/30      4.63G     0.9519      2.026      1.053        169        640: 100% 11/11 [00:05<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.97it/s]\n",
            "                   all         23        157      0.719      0.448      0.564      0.429\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/30      4.51G     0.9484      1.787       1.04        140        640: 100% 11/11 [00:04<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.66it/s]\n",
            "                   all         23        157      0.623      0.604      0.669      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/30      4.54G     0.9119      1.616      1.027        148        640: 100% 11/11 [00:05<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.88it/s]\n",
            "                   all         23        157      0.645      0.672      0.736      0.554\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/30      4.64G     0.9068      1.487      1.017        155        640: 100% 11/11 [00:03<00:00,  2.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.74it/s]\n",
            "                   all         23        157      0.616       0.71      0.777      0.566\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/30      4.63G     0.8951      1.361      1.011        208        640: 100% 11/11 [00:04<00:00,  2.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.67it/s]\n",
            "                   all         23        157      0.681      0.765      0.816      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/30      4.54G     0.8771      1.253       1.01        132        640: 100% 11/11 [00:05<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.35it/s]\n",
            "                   all         23        157      0.655        0.8      0.804      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/30      4.55G     0.8729      1.205      1.001        181        640: 100% 11/11 [00:04<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.07it/s]\n",
            "                   all         23        157      0.632      0.892      0.828       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/30      4.53G      0.842      1.093     0.9904        142        640: 100% 11/11 [00:05<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.89it/s]\n",
            "                   all         23        157      0.751      0.867      0.883      0.666\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/30      4.65G     0.8083      1.052     0.9851        134        640: 100% 11/11 [00:04<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.53it/s]\n",
            "                   all         23        157      0.737      0.938      0.901       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/30      4.53G     0.8348      1.069     0.9956        156        640: 100% 11/11 [00:04<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.37it/s]\n",
            "                   all         23        157      0.791      0.874      0.909      0.693\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/30      4.54G     0.8078     0.9732     0.9797        119        640: 100% 11/11 [00:06<00:00,  1.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.13it/s]\n",
            "                   all         23        157      0.744      0.974      0.943       0.72\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/30      4.58G     0.8181     0.9534     0.9804        177        640: 100% 11/11 [00:04<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.28it/s]\n",
            "                   all         23        157      0.827      0.916      0.953      0.737\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/30      4.53G     0.7936     0.9274     0.9801        140        640: 100% 11/11 [00:04<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.83it/s]\n",
            "                   all         23        157      0.831      0.893      0.951       0.72\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/30      5.33G     0.7542     0.9503     0.9792         94        640: 100% 11/11 [00:08<00:00,  1.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.89it/s]\n",
            "                   all         23        157       0.74      0.938      0.955      0.709\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/30       4.6G     0.7458     0.9127     0.9697         87        640: 100% 11/11 [00:05<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.57it/s]\n",
            "                   all         23        157      0.827      0.918      0.974      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/30      4.59G     0.7465     0.8761     0.9684         90        640: 100% 11/11 [00:04<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.69it/s]\n",
            "                   all         23        157      0.823      0.906      0.974      0.737\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/30       4.6G     0.7294     0.8799     0.9646         97        640: 100% 11/11 [00:03<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.64it/s]\n",
            "                   all         23        157      0.823      0.951      0.979      0.753\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/30      4.59G     0.7133     0.7991     0.9546         98        640: 100% 11/11 [00:05<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.78it/s]\n",
            "                   all         23        157      0.844      0.951      0.976      0.752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/30       4.6G     0.7004     0.7961     0.9565         91        640: 100% 11/11 [00:03<00:00,  2.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.85it/s]\n",
            "                   all         23        157      0.914      0.926      0.973      0.758\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/30      4.59G     0.7071     0.7966     0.9541         85        640: 100% 11/11 [00:03<00:00,  2.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.66it/s]\n",
            "                   all         23        157      0.891      0.953      0.973      0.758\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/30       4.6G     0.6919     0.7757     0.9497         90        640: 100% 11/11 [00:05<00:00,  1.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.84it/s]\n",
            "                   all         23        157      0.901      0.951      0.974      0.761\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/30      4.59G     0.6955     0.7927      0.952         88        640: 100% 11/11 [00:03<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  3.16it/s]\n",
            "                   all         23        157      0.874      0.955      0.974      0.752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/30       4.6G      0.696     0.7769     0.9521         95        640: 100% 11/11 [00:04<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.81it/s]\n",
            "                   all         23        157      0.921      0.923      0.983      0.759\n",
            "\n",
            "30 epochs completed in 0.060 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.3MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.3MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,011,693 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  2.53it/s]\n",
            "                   all         23        157      0.902      0.949      0.974       0.76\n",
            "               address         10         10      0.972        0.9      0.986      0.859\n",
            "                  demo          9          9      0.997          1      0.995      0.803\n",
            "                   dob          9          9      0.975          1      0.995      0.818\n",
            "                expiry          9          9      0.999          1      0.995      0.842\n",
            "             firstName         10         10          1       0.94      0.995      0.702\n",
            "            front_logo          9          9      0.971          1      0.995      0.855\n",
            "       invalid_address          2          2       0.57          1      0.995      0.821\n",
            "          invalid_demo          2          2      0.868          1      0.995      0.747\n",
            "           invalid_dob          3          3      0.937          1      0.995      0.687\n",
            "        invalid_expiry          2          2       0.88          1      0.995      0.821\n",
            "     invalid_firstName          2          2      0.914          1      0.995      0.547\n",
            "         invalid_issue          2          2      0.877          1      0.995      0.846\n",
            "           invalid_job          2          2          1      0.936      0.995      0.896\n",
            "      invalid_lastName          2          2       0.63          1      0.995      0.598\n",
            "           invalid_nid          2          2      0.862          1      0.995      0.448\n",
            "      invalid_nid_back          2          2      0.926          1      0.995      0.895\n",
            "         invalid_photo          2          2      0.391        0.5      0.497      0.448\n",
            "        invalid_serial          3          3      0.981          1      0.995      0.597\n",
            " invalid_watermark_tut          2          2      0.881          1      0.995      0.921\n",
            "                 issue          9          9          1      0.947      0.995      0.861\n",
            "                   job          9          9          1      0.848      0.984      0.766\n",
            "              lastName         10         10          1       0.92      0.995      0.744\n",
            "                   nid         10         10          1      0.927      0.995      0.791\n",
            "              nid_back          9          9      0.998          1      0.995      0.769\n",
            "                 photo         10         10      0.868        0.8      0.951      0.799\n",
            "                serial          7          7      0.864      0.916      0.978      0.692\n",
            "         watermark_tut          9          9      0.979          1      0.995      0.941\n",
            "Speed: 0.2ms preprocess, 2.7ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect model=yolov8n.pt data='/content/Egyptian-ID-Detectr-3/data.yaml' epochs=30 imgsz=640 batch=32 lr0=0.005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkYA4OoCiRvF",
        "outputId": "515e311a-2af2-4b75-a98f-e19d5b72ad92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,011,693 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Egyptian-ID-Detectr-3/valid/labels.cache... 23 images, 0 backgrounds, 0 corrupt: 100%|██████████| 23/23 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         23        157      0.901      0.951      0.974      0.763\n",
            "               address         10         10      0.972        0.9      0.986      0.874\n",
            "                  demo          9          9      0.997          1      0.995      0.803\n",
            "                   dob          9          9      0.974          1      0.995      0.814\n",
            "                expiry          9          9      0.999          1      0.995      0.842\n",
            "             firstName         10         10          1      0.942      0.995      0.713\n",
            "            front_logo          9          9       0.97          1      0.995      0.855\n",
            "       invalid_address          2          2      0.567          1      0.995      0.821\n",
            "          invalid_demo          2          2      0.867          1      0.995      0.747\n",
            "           invalid_dob          3          3      0.935          1      0.995      0.687\n",
            "        invalid_expiry          2          2      0.879          1      0.995      0.821\n",
            "     invalid_firstName          2          2      0.907          1      0.995      0.647\n",
            "         invalid_issue          2          2      0.876          1      0.995      0.846\n",
            "           invalid_job          2          2          1      0.943      0.995      0.846\n",
            "      invalid_lastName          2          2       0.63          1      0.995      0.598\n",
            "           invalid_nid          2          2      0.861          1      0.995      0.448\n",
            "      invalid_nid_back          2          2      0.924          1      0.995      0.895\n",
            "         invalid_photo          2          2      0.388        0.5      0.497      0.448\n",
            "        invalid_serial          3          3      0.981          1      0.995      0.597\n",
            " invalid_watermark_tut          2          2       0.88          1      0.995      0.921\n",
            "                 issue          9          9          1       0.95      0.995      0.861\n",
            "                   job          9          9          1      0.874      0.984      0.777\n",
            "              lastName         10         10          1      0.921      0.995      0.736\n",
            "                   nid         10         10          1      0.929      0.995      0.791\n",
            "              nid_back          9          9      0.997          1      0.995      0.768\n",
            "                 photo         10         10      0.867        0.8      0.951      0.799\n",
            "                serial          7          7      0.865      0.923      0.978      0.693\n",
            "         watermark_tut          9          9      0.978          1      0.995      0.941\n",
            "Speed: 0.3ms preprocess, 29.6ms inference, 0.0ms loss, 46.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "Precision: 0.9005\n",
            "Recall: 0.9512\n",
            "mAP@50: 0.9735\n",
            "mAP@50-95: 0.7625\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model from the specific run\n",
        "model = YOLO('/content/runs/detect/train2/weights/best.pt')\n",
        "# Evaluate on the validation dataset\n",
        "results = model.val()\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Precision: {results.box.mp:.4f}\")  # Mean Precision\n",
        "print(f\"Recall: {results.box.mr:.4f}\")     # Mean Recall\n",
        "print(f\"mAP@50: {results.box.map50:.4f}\")  # Mean Average Precision at IoU 0.5\n",
        "print(f\"mAP@50-95: {results.box.map:.4f}\") # Mean Average Precision at IoU 0.5:0.95\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model To Detect ID Numbers"
      ],
      "metadata": {
        "id": "2UAvT4_-VCyZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcySF9QMUOdD",
        "outputId": "5454489c-ab0d-448d-ce22-c88222c9776d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.39)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.2.77, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in arabic-numbers-2 to yolov8:: 100%|██████████| 87729/87729 [00:01<00:00, 56687.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to arabic-numbers-2 in yolov8:: 100%|██████████| 3602/3602 [00:00<00:00, 5822.56it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"JiN3MQkW1CkxFGPpnE0K\")\n",
        "project = rf.workspace(\"egyptian-ids\").project(\"arabic-numbers-vmdt0\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85DpLLOFURaY",
        "outputId": "5b3bd5a5-4dee-4bbf-b50b-013081731385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt to 'yolov8s.pt'...\n",
            "100% 21.5M/21.5M [00:00<00:00, 244MB/s]\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/arabic-numbers-2/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2119918  ultralytics.nn.modules.head.Detect           [10, [128, 256, 512]]         \n",
            "Model summary: 225 layers, 11,139,470 parameters, 11,139,454 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/arabic-numbers-2/train/labels... 1603 images, 39 backgrounds, 0 corrupt: 100% 1603/1603 [00:00<00:00, 1984.56it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/arabic-numbers-2/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/arabic-numbers-2/valid/labels... 150 images, 1 backgrounds, 0 corrupt: 100% 150/150 [00:00<00:00, 788.85it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/arabic-numbers-2/valid/labels.cache\n",
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      4.41G      1.502      2.046     0.9648         39        640: 100% 101/101 [00:45<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.52it/s]\n",
            "                   all        150       2084      0.973      0.947      0.981      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20      4.25G      1.289     0.7828     0.9045         65        640: 100% 101/101 [00:39<00:00,  2.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.36it/s]\n",
            "                   all        150       2084      0.978       0.98      0.987      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20      4.39G      1.264     0.7156     0.9037         28        640: 100% 101/101 [00:34<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.70it/s]\n",
            "                   all        150       2084      0.976      0.979      0.988      0.643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20      4.32G      1.252     0.6634     0.9018         56        640: 100% 101/101 [00:34<00:00,  2.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.34it/s]\n",
            "                   all        150       2084      0.976      0.978       0.99      0.668\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20      4.32G      1.208     0.6399     0.8928         38        640: 100% 101/101 [00:33<00:00,  2.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.43it/s]\n",
            "                   all        150       2084      0.983      0.983      0.991      0.682\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20      4.22G      1.188     0.6122     0.8824         51        640: 100% 101/101 [00:34<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.38it/s]\n",
            "                   all        150       2084      0.988      0.981       0.99      0.679\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20      4.31G      1.166     0.5867     0.8812         64        640: 100% 101/101 [00:34<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.22it/s]\n",
            "                   all        150       2084      0.985      0.991      0.991       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20      4.33G      1.137     0.5743     0.8754         80        640: 100% 101/101 [00:34<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.93it/s]\n",
            "                   all        150       2084      0.987      0.992      0.991      0.698\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20      4.33G      1.115     0.5541      0.873         41        640: 100% 101/101 [00:33<00:00,  3.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.80it/s]\n",
            "                   all        150       2084      0.987       0.99      0.992        0.7\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20      4.33G      1.101     0.5452     0.8723         57        640: 100% 101/101 [00:38<00:00,  2.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.09it/s]\n",
            "                   all        150       2084      0.987      0.991      0.993      0.718\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20      4.22G      1.087     0.5402     0.8854         25        640: 100% 101/101 [00:38<00:00,  2.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.04it/s]\n",
            "                   all        150       2084      0.988      0.992      0.993      0.709\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20       4.2G      1.051     0.5322     0.8761         42        640: 100% 101/101 [00:34<00:00,  2.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.98it/s]\n",
            "                   all        150       2084      0.987      0.991      0.993      0.722\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20      4.21G      1.054     0.5173     0.8738         37        640: 100% 101/101 [00:33<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.15it/s]\n",
            "                   all        150       2084      0.987      0.996      0.994      0.725\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20      4.16G      1.041      0.502     0.8736         31        640: 100% 101/101 [00:34<00:00,  2.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.88it/s]\n",
            "                   all        150       2084       0.99      0.989      0.992      0.721\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20      4.22G      1.015      0.482     0.8683         35        640: 100% 101/101 [00:33<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.86it/s]\n",
            "                   all        150       2084      0.988      0.993      0.993      0.731\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20       4.2G     0.9925     0.4757     0.8628         41        640: 100% 101/101 [00:31<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.82it/s]\n",
            "                   all        150       2084      0.988      0.994      0.993      0.742\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20       4.2G     0.9881     0.4705     0.8638         36        640: 100% 101/101 [00:31<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "                   all        150       2084      0.988      0.995      0.992      0.743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20      4.16G     0.9712     0.4587     0.8624         39        640: 100% 101/101 [00:31<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.25it/s]\n",
            "                   all        150       2084      0.988      0.995      0.993      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20      4.22G      0.958     0.4525     0.8596         41        640: 100% 101/101 [00:31<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.57it/s]\n",
            "                   all        150       2084       0.99      0.994      0.993       0.75\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20      4.21G     0.9312     0.4437     0.8549         32        640: 100% 101/101 [00:32<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.27it/s]\n",
            "                   all        150       2084      0.989      0.995      0.993      0.756\n",
            "\n",
            "20 epochs completed in 0.212 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11,129,454 parameters, 0 gradients, 28.5 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:13<00:00,  2.65s/it]\n",
            "                   all        150       2084      0.989      0.995      0.993      0.756\n",
            "                     0        149        435      0.988      0.998      0.993      0.637\n",
            "                     1        149        416      0.989      0.995      0.994      0.747\n",
            "                     2        149        361      0.985      0.997      0.993      0.781\n",
            "                     3         89        122          1      0.989      0.995      0.803\n",
            "                     4         73        100       0.97      0.986      0.979      0.702\n",
            "                     5        147        244          1      0.998      0.995      0.774\n",
            "                     6         68         80      0.997      0.988      0.995      0.782\n",
            "                     7         76        110      0.991      0.999      0.993        0.8\n",
            "                     8        113        166      0.987          1      0.995      0.797\n",
            "                     9         46         50      0.978          1      0.995      0.735\n",
            "Speed: 0.4ms preprocess, 5.2ms inference, 0.0ms loss, 10.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "#train: /content/Egyptian-ID-Detectr-3/train/images\n",
        "#val: /content/Egyptian-ID-Detectr-3/valid/images\n",
        "!yolo task=detect mode=train model=yolov8s.pt data='/content/arabic-numbers-2/data.yaml' epochs=20 imgsz=640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKO954qYjF_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba0f39c-695d-4253-bafc-53ef64df99ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.77 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11,129,454 parameters, 0 gradients, 28.5 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/arabic-numbers-2/valid/labels.cache... 150 images, 1 backgrounds, 0 corrupt: 100%|██████████| 150/150 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:12<00:00,  1.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        150       2084      0.989      0.995      0.993      0.766\n",
            "                     0        149        435      0.988      0.998      0.993      0.647\n",
            "                     1        149        416      0.989      0.995      0.994      0.752\n",
            "                     2        149        361      0.985      0.997      0.993      0.792\n",
            "                     3         89        122          1      0.989      0.995      0.816\n",
            "                     4         73        100       0.97      0.985      0.979      0.709\n",
            "                     5        147        244          1      0.998      0.995       0.78\n",
            "                     6         68         80      0.997      0.988      0.995      0.795\n",
            "                     7         76        110      0.991      0.999      0.993      0.813\n",
            "                     8        113        166      0.987          1      0.995       0.81\n",
            "                     9         46         50      0.978          1      0.995      0.745\n",
            "Speed: 5.5ms preprocess, 13.2ms inference, 0.0ms loss, 6.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
            "Precision: 0.9887\n",
            "Recall: 0.9948\n",
            "mAP@50: 0.9927\n",
            "mAP@50-95: 0.7659\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model from the specific run\n",
        "model = YOLO('/content/runs/detect/train3/weights/best.pt')\n",
        "# Evaluate on the validation dataset\n",
        "results = model.val()\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Precision: {results.box.mp:.4f}\")  # Mean Precision\n",
        "print(f\"Recall: {results.box.mr:.4f}\")     # Mean Recall\n",
        "print(f\"mAP@50: {results.box.map50:.4f}\")  # Mean Average Precision at IoU 0.5\n",
        "print(f\"mAP@50-95: {results.box.map:.4f}\") # Mean Average Precision at IoU 0.5:0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model To Detect ID Card"
      ],
      "metadata": {
        "id": "8FyTTCD1VfON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"JiN3MQkW1CkxFGPpnE0K\")\n",
        "project = rf.workspace(\"iddetection-zr0sa\").project(\"national-id-ltfb6\")\n",
        "version = project.version(7)\n",
        "dataset = version.download(\"yolov8\")\n"
      ],
      "metadata": {
        "id": "SrSyBzKVSJit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0275892-110e-463c-8073-3facb9b9f11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.39-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Downloading roboflow-1.1.39-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, python-dotenv, requests-toolbelt, roboflow\n",
            "Successfully installed filetype-1.2.0 python-dotenv-1.0.1 requests-toolbelt-1.0.0 roboflow-1.1.39\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.2.78, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in National-ID-7 to yolov8:: 100%|██████████| 76857/76857 [00:03<00:00, 24501.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to National-ID-7 in yolov8:: 100%|██████████| 3874/3874 [00:02<00:00, 1643.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xJqrK57mkLcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8n.pt data='/content/National-ID-7/data.yaml' epochs=40 imgsz=640"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Op6tF1itP_F",
        "outputId": "171259a7-dad7-4196-a956-91c717524c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.25M/6.25M [00:00<00:00, 116MB/s]\n",
            "Ultralytics YOLOv8.2.78 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/National-ID-7/data.yaml, epochs=40, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 20.4MB/s]\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/National-ID-7/train/labels... 1668 images, 0 backgrounds, 0 corrupt: 100% 1668/1668 [00:00<00:00, 2010.55it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/National-ID-7/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/National-ID-7/valid/labels... 172 images, 0 backgrounds, 0 corrupt: 100% 172/172 [00:00<00:00, 756.77it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/National-ID-7/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 40 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/40      2.64G     0.6359      2.516      1.069          9        640: 100% 105/105 [00:37<00:00,  2.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:04<00:00,  1.32it/s]\n",
            "                   all        172        201       0.79      0.532      0.653      0.545\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/40      2.32G     0.5694      1.401      1.023          9        640: 100% 105/105 [00:32<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.54it/s]\n",
            "                   all        172        201      0.665      0.765       0.76      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/40      2.29G     0.5618      1.213      1.008          8        640: 100% 105/105 [00:33<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.68it/s]\n",
            "                   all        172        201      0.633      0.811       0.78      0.658\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/40       2.3G     0.5591      1.056      1.006         12        640: 100% 105/105 [00:34<00:00,  3.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.56it/s]\n",
            "                   all        172        201      0.731      0.733      0.827      0.709\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/40      2.31G     0.5612     0.9438      1.002          8        640: 100% 105/105 [00:37<00:00,  2.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.37it/s]\n",
            "                   all        172        201      0.732      0.788      0.848      0.752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/40      2.31G     0.5222     0.8092     0.9748         16        640: 100% 105/105 [00:31<00:00,  3.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.25it/s]\n",
            "                   all        172        201      0.807      0.883      0.894      0.807\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/40      2.29G     0.5119     0.7364     0.9789          6        640: 100% 105/105 [00:31<00:00,  3.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:03<00:00,  1.98it/s]\n",
            "                   all        172        201      0.902      0.903      0.952      0.846\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/40      2.32G     0.5102     0.6983     0.9758         16        640: 100% 105/105 [00:31<00:00,  3.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.42it/s]\n",
            "                   all        172        201      0.909      0.899      0.951      0.842\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/40      2.32G     0.4964     0.6495     0.9645         15        640: 100% 105/105 [00:34<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.87it/s]\n",
            "                   all        172        201       0.94      0.959      0.968      0.862\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/40       2.3G     0.5037     0.6256      0.972         12        640: 100% 105/105 [00:32<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.92it/s]\n",
            "                   all        172        201      0.856      0.926      0.946      0.865\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/40       2.3G     0.4756     0.5755     0.9583         13        640: 100% 105/105 [00:32<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.90it/s]\n",
            "                   all        172        201      0.912      0.979      0.977      0.881\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/40      2.32G      0.477     0.5576       0.96          8        640: 100% 105/105 [00:33<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.89it/s]\n",
            "                   all        172        201      0.909      0.934       0.97      0.862\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/40       2.3G     0.4834      0.545     0.9555         10        640: 100% 105/105 [00:36<00:00,  2.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.96it/s]\n",
            "                   all        172        201       0.94       0.97      0.975        0.9\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/40      2.31G     0.4695     0.5136     0.9578         16        640: 100% 105/105 [00:33<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.17it/s]\n",
            "                   all        172        201      0.957      0.983      0.978      0.908\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/40      2.31G     0.4569     0.5144     0.9496         10        640: 100% 105/105 [00:32<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.98it/s]\n",
            "                   all        172        201      0.901      0.958      0.965      0.882\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/40      2.29G       0.46     0.4936     0.9502         11        640: 100% 105/105 [00:32<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.75it/s]\n",
            "                   all        172        201      0.946      0.964       0.97      0.887\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/40      2.31G     0.4626     0.4843     0.9462          9        640: 100% 105/105 [00:36<00:00,  2.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.06it/s]\n",
            "                   all        172        201      0.955       0.93      0.973       0.89\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/40      2.32G     0.4593      0.474     0.9516         16        640: 100% 105/105 [00:32<00:00,  3.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.64it/s]\n",
            "                   all        172        201      0.941      0.941      0.973      0.885\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/40      2.29G     0.4494     0.4529     0.9485          5        640: 100% 105/105 [00:33<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.70it/s]\n",
            "                   all        172        201      0.953      0.956      0.982      0.907\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/40       2.3G     0.4393     0.4391     0.9453          8        640: 100% 105/105 [00:32<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.01it/s]\n",
            "                   all        172        201      0.945      0.974      0.978      0.909\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/40       2.3G     0.4409     0.4418     0.9403          9        640: 100% 105/105 [00:33<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.06it/s]\n",
            "                   all        172        201      0.965      0.972      0.981      0.898\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/40      2.31G      0.443     0.4398     0.9389          8        640: 100% 105/105 [00:32<00:00,  3.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.96it/s]\n",
            "                   all        172        201      0.952       0.98      0.977      0.904\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/40      2.29G      0.436     0.4166     0.9377         13        640: 100% 105/105 [00:32<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.63it/s]\n",
            "                   all        172        201      0.936      0.959      0.968      0.897\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/40      2.29G     0.4284     0.4016      0.938         15        640: 100% 105/105 [00:32<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.05it/s]\n",
            "                   all        172        201      0.944      0.974       0.98      0.914\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/40      2.31G     0.4261     0.3916     0.9323         13        640: 100% 105/105 [00:32<00:00,  3.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.94it/s]\n",
            "                   all        172        201      0.971      0.963      0.988      0.917\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/40      2.32G     0.4208     0.4019       0.93         13        640: 100% 105/105 [00:35<00:00,  2.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.96it/s]\n",
            "                   all        172        201      0.958      0.968       0.97      0.901\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/40      2.29G     0.4169     0.3831     0.9292         11        640: 100% 105/105 [00:32<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.80it/s]\n",
            "                   all        172        201      0.954      0.977      0.984      0.922\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/40       2.3G      0.414     0.3749     0.9247         14        640: 100% 105/105 [00:33<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.99it/s]\n",
            "                   all        172        201      0.966      0.983      0.988      0.924\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/40      2.31G      0.411     0.3701     0.9321         13        640: 100% 105/105 [00:33<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.55it/s]\n",
            "                   all        172        201      0.956      0.958      0.981      0.921\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/40      2.31G      0.412     0.3602     0.9251         11        640: 100% 105/105 [00:36<00:00,  2.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.60it/s]\n",
            "                   all        172        201       0.94      0.974      0.975      0.918\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      31/40      2.29G     0.3355     0.3464     0.8782          3        640: 100% 105/105 [00:32<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.59it/s]\n",
            "                   all        172        201      0.955      0.974      0.979      0.922\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      32/40      2.29G     0.3303     0.3268     0.8846          3        640: 100% 105/105 [00:29<00:00,  3.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.35it/s]\n",
            "                   all        172        201      0.968      0.981      0.989      0.933\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      33/40       2.3G     0.3366       0.32     0.8835          4        640: 100% 105/105 [00:28<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:02<00:00,  2.97it/s]\n",
            "                   all        172        201      0.964      0.971      0.989      0.934\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      34/40       2.3G     0.3187     0.3027     0.8682          4        640: 100% 105/105 [00:33<00:00,  3.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.94it/s]\n",
            "                   all        172        201      0.957       0.97       0.98      0.929\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      35/40      2.29G      0.312     0.2851     0.8795          3        640: 100% 105/105 [00:31<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.01it/s]\n",
            "                   all        172        201       0.96      0.969      0.985      0.943\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      36/40      2.29G     0.3217     0.3031     0.8599          2        640: 100% 105/105 [00:34<00:00,  3.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  4.15it/s]\n",
            "                   all        172        201      0.981      0.973       0.98      0.935\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      37/40       2.3G     0.2971     0.2761      0.862          4        640: 100% 105/105 [00:31<00:00,  3.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.62it/s]\n",
            "                   all        172        201      0.969      0.977       0.99      0.939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      38/40       2.3G     0.2939     0.2653       0.86          7        640: 100% 105/105 [00:50<00:00,  2.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 6/6 [00:01<00:00,  3.93it/s]\n",
            "                   all        172        201      0.975      0.979      0.994      0.947\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      39/40      2.26G     0.3012      0.249      0.851         16        640:  30% 32/105 [00:10<00:34,  2.12it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the trained model from the specific run\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "# Evaluate on the validation dataset\n",
        "results = model.val()\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Precision: {results.box.mp:.4f}\")  # Mean Precision\n",
        "print(f\"Recall: {results.box.mr:.4f}\")     # Mean Recall\n",
        "print(f\"mAP@50: {results.box.map50:.4f}\")  # Mean Average Precision at IoU 0.5\n",
        "print(f\"mAP@50-95: {results.box.map:.4f}\") # Mean Average Precision at IoU 0.5:0.95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fkZRNnPtzQw",
        "outputId": "9988642e-1c6e-45f6-995d-08a2157dc023"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.78 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/National-ID-7/valid/labels.cache... 172 images, 0 backgrounds, 0 corrupt: 100%|██████████| 172/172 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 11/11 [00:03<00:00,  2.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        172        201      0.975      0.979      0.994      0.949\n",
            "           back-bottom         10         10      0.901      0.909      0.986      0.946\n",
            "             back-left         12         12      0.992          1      0.995      0.931\n",
            "            back-right         14         14          1      0.937      0.995       0.96\n",
            "               back-up         57         57          1      0.984      0.995      0.947\n",
            "          front-bottom         18         18      0.997          1      0.995      0.934\n",
            "            front-left         12         12      0.972          1      0.995      0.978\n",
            "           front-right         19         19      0.941          1      0.995       0.93\n",
            "              front-up         54         59      0.999          1      0.995      0.965\n",
            "Speed: 2.6ms preprocess, 5.9ms inference, 0.0ms loss, 5.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
            "Precision: 0.9754\n",
            "Recall: 0.9787\n",
            "mAP@50: 0.9939\n",
            "mAP@50-95: 0.9489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxb6ayAbYHMa",
        "outputId": "5ce0d831-7940-4852-ec94-27a33d25e2a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y tesseract-ocr-ara  # Install Arabic language support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ4nQ-xeYisf",
        "outputId": "45b4eb9b-bfbe-442c-ac6d-4ccd175bc85f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [945 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Ign:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,556 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,498 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,425 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,234 kB]\n",
            "Fetched 16.1 MB in 6s (2,922 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 3s (1,850 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-ara\n",
            "0 upgraded, 1 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 645 kB of archives.\n",
            "After this operation, 1,447 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ara all 1:4.00~git30-7274cfa-1.1 [645 kB]\n",
            "Fetched 645 kB in 2s (371 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-ara.\n",
            "(Reading database ... 123642 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-ara_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-ara (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHayxpesYoOn",
        "outputId": "31d4ec5e-1c0b-448a-da55-2f9e1ad078f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.81-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.5-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.81-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.4/869.4 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading ultralytics_thop-2.0.5-py3-none-any.whl (25 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.81 ultralytics-thop-2.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import important libraries"
      ],
      "metadata": {
        "id": "ppgh2zicYvoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import re\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from PIL import Image as PILImage\n"
      ],
      "metadata": {
        "id": "C6mc36BjYuan"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face of id card without need to rotate .\n"
      ],
      "metadata": {
        "id": "ifcMhoGKa7oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to preprocess the cropped image\n",
        "def preprocess_image(cropped_image):\n",
        "    gray = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "    return binary\n",
        "\n",
        "# Functions for specific fields with custom OCR configurations\n",
        "def extract_text(image, bbox, lang='ara'):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cropped_image = image[y1:y2, x1:x2]\n",
        "    preprocessed_image = preprocess_image(cropped_image)\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(preprocessed_image, lang=lang, config=custom_config)\n",
        "    return text.strip()\n",
        "\n",
        "# Function to detect national ID numbers in a cropped image\n",
        "def detect_national_id(cropped_image):\n",
        "    model = YOLO('/content/detect_id.pt')  # Load the model directly in the function\n",
        "    results = model(cropped_image)\n",
        "    detected_info = []\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cls = int(box.cls)\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            detected_info.append((cls, x1))\n",
        "            cv2.rectangle(cropped_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(cropped_image, str(cls), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "    detected_info.sort(key=lambda x: x[1])\n",
        "    id_number = ''.join([str(cls) for cls, _ in detected_info])\n",
        "\n",
        "    cv2_imshow(cropped_image)\n",
        "    return id_number\n",
        "\n",
        "# Function to remove numbers from a string\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Function to plot image with bounding boxes\n",
        "def plot_image_with_boxes(image, boxes):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    ax = plt.gca()\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "# Function to expand bounding box height only\n",
        "def expand_bbox_height(bbox, scale=1.2, image_shape=None):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    width = x2 - x1\n",
        "    height = y2 - y1\n",
        "    center_x = x1 + width // 2\n",
        "    center_y = y1 + height // 2\n",
        "    new_height = int(height * scale)\n",
        "    new_y1 = max(center_y - new_height // 2, 0)\n",
        "    new_y2 = min(center_y + new_height // 2, image_shape[0])\n",
        "    return [x1, new_y1, x2, new_y2]\n",
        "\n",
        "# Function to process the cropped image\n",
        "def process_image(cropped_image):\n",
        "    # Load the trained YOLO model for objects (fields) detection\n",
        "    model = YOLO('/content/detect_odjects.pt')\n",
        "    results = model(cropped_image)\n",
        "\n",
        "    # Variables to store extracted values\n",
        "    first_name = ''\n",
        "    second_name = ''\n",
        "    merged_name = ''\n",
        "    nid = ''\n",
        "    address = ''\n",
        "    serial = ''\n",
        "\n",
        "    # Loop through the results\n",
        "    for result in results:\n",
        "        result.show()  # Shows the image with bounding boxes\n",
        "        output_path = '/content/d2.jpg'\n",
        "        result.save(output_path)\n",
        "        display(Image(filename=output_path))\n",
        "\n",
        "        boxes = [box.xyxy[0].tolist() for box in result.boxes]\n",
        "        plot_image_with_boxes(cropped_image, boxes)\n",
        "\n",
        "        for box in result.boxes:\n",
        "            bbox = box.xyxy[0].tolist()\n",
        "            class_id = int(box.cls[0].item())\n",
        "            class_name = result.names[class_id]\n",
        "            bbox = [int(coord) for coord in bbox]\n",
        "\n",
        "            if class_name == 'firstName':\n",
        "                first_name = extract_text(cropped_image, bbox, lang='ara')\n",
        "            elif class_name == 'lastName':\n",
        "                second_name = extract_text(cropped_image, bbox, lang='ara')\n",
        "            elif class_name == 'serial':\n",
        "                serial = extract_text(cropped_image, bbox, lang='eng')\n",
        "            elif class_name == 'address':\n",
        "                address = extract_text(cropped_image, bbox, lang='ara')\n",
        "                address = remove_numbers(address)\n",
        "            elif class_name == 'nid':\n",
        "                expanded_bbox = expand_bbox_height(bbox, scale=1.5, image_shape=cropped_image.shape)\n",
        "                cropped_nid = cropped_image[expanded_bbox[1]:expanded_bbox[3], expanded_bbox[0]:expanded_bbox[2]]\n",
        "                nid = detect_national_id(cropped_nid)\n",
        "\n",
        "    merged_name = f\"{first_name} {second_name}\"\n",
        "    print(f\"First Name: {first_name}\")\n",
        "    print(f\"Second Name: {second_name}\")\n",
        "    print(f\"Full Name: {merged_name}\")\n",
        "    print(f\"National ID: {nid}\")\n",
        "    print(f\"Address: {address}\")\n",
        "    print(f\"Serial: {serial}\")\n",
        "\n",
        "    decoded_info = decode_egyptian_id(nid)\n",
        "    for key, value in decoded_info.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Function to decode the Egyptian ID number\n",
        "def decode_egyptian_id(id_number):\n",
        "    governorates = {\n",
        "        '01': 'Cairo',\n",
        "        '02': 'Alexandria',\n",
        "        '03': 'Port Said',\n",
        "        '04': 'Suez',\n",
        "        '11': 'Damietta',\n",
        "        '12': 'Dakahlia',\n",
        "        '13': 'Ash Sharqia',\n",
        "        '14': 'Kaliobeya',\n",
        "        '15': 'Kafr El - Sheikh',\n",
        "        '16': 'Gharbia',\n",
        "        '17': 'Monoufia',\n",
        "        '18': 'El Beheira',\n",
        "        '19': 'Ismailia',\n",
        "        '21': 'Giza',\n",
        "        '22': 'Beni Suef',\n",
        "        '23': 'Fayoum',\n",
        "        '24': 'El Menia',\n",
        "        '25': 'Assiut',\n",
        "        '26': 'Sohag',\n",
        "        '27': 'Qena',\n",
        "        '28': 'Aswan',\n",
        "        '29': 'Luxor',\n",
        "        '31': 'Red Sea',\n",
        "        '32': 'New Valley',\n",
        "        '33': 'Matrouh',\n",
        "        '34': 'North Sinai',\n",
        "        '35': 'South Sinai',\n",
        "        '88': 'Foreign'\n",
        "    }\n",
        "\n",
        "    if len(id_number) != 14:\n",
        "        raise ValueError(\"ID number must be 14 digits long\")\n",
        "\n",
        "    century_digit = int(id_number[0])\n",
        "    year = int(id_number[1:3])\n",
        "    month = int(id_number[3:5])\n",
        "    day = int(id_number[5:7])\n",
        "    governorate_code = id_number[7:9]\n",
        "    gender_code = int(id_number[12:13])\n",
        "\n",
        "    if century_digit == 2:\n",
        "        century = \"1900-1999\"\n",
        "        full_year = 1900 + year\n",
        "    elif century_digit == 3:\n",
        "        century = \"2000-2099\"\n",
        "        full_year = 2000 + year\n",
        "    else:\n",
        "        raise ValueError(\"Invalid century digit\")\n",
        "\n",
        "    gender = \"Male\" if gender_code % 2 != 0 else \"Female\"\n",
        "    governorate = governorates.get(governorate_code, \"Unknown\")\n",
        "    birth_date = f\"{full_year:04d}-{month:02d}-{day:02d}\"\n",
        "\n",
        "    return {\n",
        "        'Birth Date': birth_date,\n",
        "        'Governorate': governorate,\n",
        "        'Gender': gender\n",
        "    }\n",
        "\n",
        "# Function to detect the ID card and pass it to the existing code\n",
        "def detect_and_process_id_card(image_path):\n",
        "    # Load the ID card detection model\n",
        "    id_card_model = YOLO('/content/detect_id_card.pt')\n",
        "\n",
        "    # Perform inference to detect the ID card\n",
        "    id_card_results = id_card_model(image_path)\n",
        "\n",
        "    # Load the original image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Crop the ID card from the image\n",
        "    for result in id_card_results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
        "            cropped_image = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Pass the cropped image to the existing processing function\n",
        "    process_image(cropped_image)\n",
        "\n",
        "# Example usage: Upload an image from your local device\n",
        "uploaded = files.upload()  # Upload the image\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    detect_and_process_id_card(filename)  # Use the uploaded image\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "EvSokdyzWmmG",
        "outputId": "d2f5bc97-4ce1-450d-ccdd-4f242e3ce50b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7c8502e-995b-42c7-8399-8477d816557d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f7c8502e-995b-42c7-8399-8477d816557d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face of id card if need to rotate .\n"
      ],
      "metadata": {
        "id": "WlwFKvRFbVUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the cropped image\n",
        "def preprocess_image(cropped_image):\n",
        "    gray = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "    return binary\n",
        "\n",
        "# Functions for specific fields with custom OCR configurations\n",
        "def extract_text(image, bbox, lang='ara'):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cropped_image = image[y1:y2, x1:x2]\n",
        "    preprocessed_image = preprocess_image(cropped_image)\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(preprocessed_image, lang=lang, config=custom_config)\n",
        "    return text.strip()\n",
        "\n",
        "# Function to detect national ID numbers in a cropped image\n",
        "def detect_national_id(cropped_image):\n",
        "    model = YOLO('/content/detect_id.pt')  # Load the model directly in the function\n",
        "    results = model(cropped_image)\n",
        "    detected_info = []\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cls = int(box.cls)\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            detected_info.append((cls, x1))\n",
        "            cv2.rectangle(cropped_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(cropped_image, str(cls), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "    detected_info.sort(key=lambda x: x[1])\n",
        "    id_number = ''.join([str(cls) for cls, _ in detected_info])\n",
        "\n",
        "    cv2_imshow(cropped_image)\n",
        "    return id_number\n",
        "\n",
        "# Function to remove numbers from a string\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Function to plot image with bounding boxes\n",
        "def plot_image_with_boxes(image, boxes):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    ax = plt.gca()\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "# Function to expand bounding box height only\n",
        "def expand_bbox_height(bbox, scale=1.2, image_shape=None):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    width = x2 - x1\n",
        "    height = y2 - y1\n",
        "    center_x = x1 + width // 2\n",
        "    center_y = y1 + height // 2\n",
        "    new_height = int(height * scale)\n",
        "    new_y1 = max(center_y - new_height // 2, 0)\n",
        "    new_y2 = min(center_y + new_height // 2, image_shape[0])\n",
        "    return [x1, new_y1, x2, new_y2]\n",
        "\n",
        "# Function to process the cropped image\n",
        "def process_image(cropped_image):\n",
        "    # Load the trained YOLO model for objects (fields) detection\n",
        "    model = YOLO('/content/detect_odjects.pt')\n",
        "    results = model(cropped_image)\n",
        "\n",
        "    # Variables to store extracted values\n",
        "    first_name = ''\n",
        "    second_name = ''\n",
        "    merged_name = ''\n",
        "    nid = ''\n",
        "    address = ''\n",
        "    serial = ''\n",
        "\n",
        "    # Loop through the results\n",
        "    for result in results:\n",
        "        result.show()  # Shows the image with bounding boxes\n",
        "        output_path = '/content/d2.jpg'\n",
        "        result.save(output_path)\n",
        "        display(Image(filename=output_path))\n",
        "\n",
        "        boxes = [box.xyxy[0].tolist() for box in result.boxes]\n",
        "        plot_image_with_boxes(cropped_image, boxes)\n",
        "\n",
        "        for box in result.boxes:\n",
        "            bbox = box.xyxy[0].tolist()\n",
        "            class_id = int(box.cls[0].item())\n",
        "            class_name = result.names[class_id]\n",
        "            bbox = [int(coord) for coord in bbox]\n",
        "\n",
        "            if class_name == 'firstName':\n",
        "                first_name = extract_text(cropped_image, bbox, lang='ara')\n",
        "            elif class_name == 'lastName':\n",
        "                second_name = extract_text(cropped_image, bbox, lang='ara')\n",
        "            elif class_name == 'serial':\n",
        "                serial = extract_text(cropped_image, bbox, lang='eng')\n",
        "            elif class_name == 'address':\n",
        "                address = extract_text(cropped_image, bbox, lang='ara')\n",
        "                address = remove_numbers(address)\n",
        "            elif class_name == 'nid':\n",
        "                expanded_bbox = expand_bbox_height(bbox, scale=1.5, image_shape=cropped_image.shape)\n",
        "                cropped_nid = cropped_image[expanded_bbox[1]:expanded_bbox[3], expanded_bbox[0]:expanded_bbox[2]]\n",
        "                nid = detect_national_id(cropped_nid)\n",
        "\n",
        "    merged_name = f\"{first_name} {second_name}\"\n",
        "    print(f\"First Name: {first_name}\")\n",
        "    print(f\"Second Name: {second_name}\")\n",
        "    print(f\"Full Name: {merged_name}\")\n",
        "    print(f\"National ID: {nid}\")\n",
        "    print(f\"Address: {address}\")\n",
        "    print(f\"Serial: {serial}\")\n",
        "\n",
        "    decoded_info = decode_egyptian_id(nid)\n",
        "    for key, value in decoded_info.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Function to decode the Egyptian ID number\n",
        "def decode_egyptian_id(id_number):\n",
        "    governorates = {\n",
        "        '01': 'Cairo',\n",
        "        '02': 'Alexandria',\n",
        "        '03': 'Port Said',\n",
        "        '04': 'Suez',\n",
        "        '11': 'Damietta',\n",
        "        '12': 'Dakahlia',\n",
        "        '13': 'Ash Sharqia',\n",
        "        '14': 'Kaliobeya',\n",
        "        '15': 'Kafr El - Sheikh',\n",
        "        '16': 'Gharbia',\n",
        "        '17': 'Monoufia',\n",
        "        '18': 'El Beheira',\n",
        "        '19': 'Ismailia',\n",
        "        '21': 'Giza',\n",
        "        '22': 'Beni Suef',\n",
        "        '23': 'Fayoum',\n",
        "        '24': 'El Menia',\n",
        "        '25': 'Assiut',\n",
        "        '26': 'Sohag',\n",
        "        '27': 'Qena',\n",
        "        '28': 'Aswan',\n",
        "        '29': 'Luxor',\n",
        "        '31': 'Red Sea',\n",
        "        '32': 'New Valley',\n",
        "        '33': 'Matrouh',\n",
        "        '34': 'North Sinai',\n",
        "        '35': 'South Sinai',\n",
        "        '88': 'Foreign'\n",
        "    }\n",
        "\n",
        "    if len(id_number) != 14:\n",
        "        raise ValueError(\"ID number must be 14 digits long\")\n",
        "\n",
        "    century_digit = int(id_number[0])\n",
        "    year = int(id_number[1:3])\n",
        "    month = int(id_number[3:5])\n",
        "    day = int(id_number[5:7])\n",
        "    governorate_code = id_number[7:9]\n",
        "    gender_code = int(id_number[12:13])\n",
        "\n",
        "    if century_digit == 2:\n",
        "        century = \"1900-1999\"\n",
        "        full_year = 1900 + year\n",
        "    elif century_digit == 3:\n",
        "        century = \"2000-2099\"\n",
        "        full_year = 2000 + year\n",
        "    else:\n",
        "        raise ValueError(\"Invalid century digit\")\n",
        "\n",
        "    gender = \"Male\" if gender_code % 2 != 0 else \"Female\"\n",
        "    governorate = governorates.get(governorate_code, \"Unknown\")\n",
        "    birth_date = f\"{full_year:04d}-{month:02d}-{day:02d}\"\n",
        "\n",
        "    return {\n",
        "        'Birth Date': birth_date,\n",
        "        'Governorate': governorate,\n",
        "        'Gender': gender\n",
        "    }\n",
        "\n",
        "# Function to process image rotation before further processing\n",
        "def process_detected_image(image_path):\n",
        "    # Load the trained model\n",
        "    model = YOLO('/content/detect_id_card.pt')\n",
        "\n",
        "    # Perform inference on the image\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Load the original image\n",
        "    img = PILImage.open(image_path)\n",
        "\n",
        "    # Initialize variable for rotated image\n",
        "    rotated_img = img.copy()  # Make a copy of the image\n",
        "\n",
        "    # Loop through the results\n",
        "    for result in results:\n",
        "        # Loop through the detected objects\n",
        "        for box in result.boxes:\n",
        "            # Get the class name\n",
        "            class_name = result.names[box.cls[0].item()]\n",
        "            print(f\"Detected Class: {class_name}\")\n",
        "\n",
        "            # Rotate the image based on the class name\n",
        "            if class_name == \"front-right\":\n",
        "                rotated_img = img.rotate(-90, expand=True)  # Rotate 90 degrees to the right\n",
        "                print(f\"Rotated image 90 degrees to the right for class: {class_name}\")\n",
        "            elif class_name == \"front-left\":\n",
        "                rotated_img = img.rotate(90, expand=True)  # Rotate 90 degrees to the left\n",
        "                print(f\"Rotated image 90 degrees to the left for class: {class_name}\")\n",
        "            elif class_name == \"front-bottom\":\n",
        "                rotated_img = img.rotate(180, expand=True)  # Rotate 180 degrees\n",
        "                print(f\"Rotated image 180 degrees for class: {class_name}\")\n",
        "            elif class_name == \"front-up\":\n",
        "                rotated_img = img.rotate(0, expand=True)  # Rotate 180 degrees\n",
        "                print(f\"image doesn't need to rotate: {class_name}\")\n",
        "\n",
        "    # Convert the PIL image back to OpenCV format\n",
        "    rotated_image_cv2 = cv2.cvtColor(np.array(rotated_img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Display the rotated image\n",
        "    display(rotated_img)\n",
        "\n",
        "    return rotated_image_cv2\n",
        "\n",
        "# Function to detect the ID card and pass it to the existing code\n",
        "def detect_and_process_id_card(image_path):\n",
        "    # Process the detected image for rotation\n",
        "    rotated_image = process_detected_image(image_path)\n",
        "\n",
        "    # Crop the ID card from the rotated image\n",
        "    id_card_model = YOLO('/content/detect_id_card.pt')\n",
        "    id_card_results = id_card_model(rotated_image)\n",
        "\n",
        "    # Load the original image using OpenCV\n",
        "    for result in id_card_results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
        "            cropped_image = rotated_image[y1:y2, x1:x2]\n",
        "\n",
        "    # Pass the cropped image to the existing processing function\n",
        "    process_image(cropped_image)\n",
        "\n",
        "# Example usage: Upload an image from your local device\n",
        "uploaded = files.upload()  # Upload the image\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    detect_and_process_id_card(filename)  # Use the uploaded image"
      ],
      "metadata": {
        "id": "AQphs3Q0bXqh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "5ccc62ae-7c86-4d41-bd7b-d1e78d910549"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-abfc8c8a-ffe7-45d1-88f6-774ae7d6c3ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-abfc8c8a-ffe7-45d1-88f6-774ae7d6c3ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back of id card"
      ],
      "metadata": {
        "id": "B-miSAorbdJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the YOLO models\n",
        "model_id_card = YOLO('/content/detect_id_card.pt')  # Model for detecting ID cards\n",
        "model_job = YOLO('/content/detect_odjects.pt')  # Model for extracting objects field\n",
        "\n",
        "# Function to extract text using Tesseract OCR\n",
        "def extract_text(image, bbox, lang='ara'):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cropped_image = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Visualize the cropped region to verify it covers the correct area\n",
        "    cv2_imshow(cropped_image)\n",
        "\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(cropped_image, lang=lang, config=custom_config)\n",
        "    return text.strip()\n",
        "\n",
        "# Function to plot image with bounding boxes\n",
        "def plot_image_with_boxes(image, boxes):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    ax = plt.gca()\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
        "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "    plt.show()\n",
        "\n",
        "# Function to detect ID card using the ID card detection model\n",
        "def detect_id_card(image_path, model):\n",
        "    results = model(image_path)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    id_card_bboxes = []\n",
        "    for result in results:\n",
        "        # Extract bounding boxes\n",
        "        id_card_bboxes.extend([box.xyxy[0].tolist() for box in result.boxes])\n",
        "\n",
        "    return image, id_card_bboxes\n",
        "\n",
        "# Function to process ID card image and extract job field\n",
        "def process_image(image_path, model_id_card, model_job):\n",
        "    # Detect ID card\n",
        "    image, id_card_bboxes = detect_id_card(image_path, model_id_card)\n",
        "\n",
        "    # Assuming only one ID card detected, process the first one\n",
        "    if id_card_bboxes:\n",
        "        bbox = id_card_bboxes[0]  # Get the first detected ID card bbox\n",
        "\n",
        "        # Crop the ID card region\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in bbox]\n",
        "        id_card_image = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Use the cropped ID card image as input for the job detection model\n",
        "        results = model_job(id_card_image)  # Pass the cropped image directly\n",
        "\n",
        "        # Loop through the results\n",
        "        job = ''\n",
        "        for result in results:\n",
        "            # Display the results\n",
        "            result.show()  # Shows the image with bounding boxes\n",
        "\n",
        "            # Save the output image\n",
        "            output_path = '/content/job_output.jpg'\n",
        "            result.save(output_path)\n",
        "\n",
        "            # Visualize the saved output image\n",
        "            display(Image(filename=output_path))\n",
        "\n",
        "            # Extract bounding boxes and class names\n",
        "            boxes = [box.xyxy[0].tolist() for box in result.boxes]\n",
        "            plot_image_with_boxes(id_card_image, boxes)  # Use id_card_image for plotting\n",
        "\n",
        "            # Extract text for the \"job\" field\n",
        "            for box in result.boxes:\n",
        "                bbox = box.xyxy[0].tolist()  # Get bounding box coordinates\n",
        "                class_id = int(box.cls[0].item())  # Get class ID\n",
        "                class_name = result.names[class_id]  # Get class name\n",
        "\n",
        "                # Convert bounding box coordinates to integers\n",
        "                bbox = [int(coord) for coord in bbox]\n",
        "\n",
        "                if class_name == 'job':\n",
        "                    job = extract_text(id_card_image, bbox, lang='ara')\n",
        "\n",
        "        # Print the extracted value\n",
        "        print(f\"Job: {job}\")\n",
        "    else:\n",
        "        print(\"No ID card detected\")\n",
        "\n",
        "# Example usage: Upload an image from your local device\n",
        "uploaded = files.upload()  # Upload the image\n",
        "for filename in uploaded.keys():\n",
        "    process_image(filename, model_id_card, model_job)  # Use the uploaded image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "IhBAat7EbDq_",
        "outputId": "9af63181-7675-45e9-930c-d8bbd8103563"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-74a6397f-413e-429c-b7d1-7080470dc229\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-74a6397f-413e-429c-b7d1-7080470dc229\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RANTY0EY1M9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}